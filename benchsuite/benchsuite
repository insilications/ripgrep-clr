#!/usr/bin/env python3

'''
benchsuite is a benchmark runner for comparing command line search tools.
'''
import shutup
shutup.please()

import argparse
import csv
import os
import os.path as path
from multiprocessing import cpu_count
import re
import shutil
import statistics
import subprocess
import sys
import time
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
from matplotlib.ticker import FormatStrFormatter
import matplotlib.ticker
import seaborn as sns
import pandas as pd
from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes
from mpl_toolkits.axes_grid1.inset_locator import mark_inset
import configparser
from simple_file_checksum import get_checksum
import hashlib

import warnings
warnings.filterwarnings("ignore")

# Some constants for identifying the corpora we use to run tests.
# We establish two very different kinds of corpora: a small number of large
# files and a large number of small files. These are vastly different use cases
# not only because of their performance characteristics, but also the
# strategies used to increase the relevance of results returned.

SUBTITLES_DIR = 'subtitles'
SUBTITLES_EN_NAME = 'en.txt'
SUBTITLES_EN_NAME16 = 'en16.txt'
SUBTITLES_EN_NAME_SAMPLE = 'en.txt'
SUBTITLES_EN_NAME_GZ = '%s.gz' % SUBTITLES_EN_NAME
# SUBTITLES_EN_URL = 'http://opus.lingfil.uu.se/OpenSubtitles2016/mono/OpenSubtitles2016.raw.en.gz'  # noqa
SUBTITLES_EN_URL = './en.txt.gz'  # noqa
SUBTITLES_RU_NAME = 'ru.txt'
SUBTITLES_RU_NAME_GZ = '%s.gz' % SUBTITLES_RU_NAME
# SUBTITLES_RU_URL = 'http://opus.lingfil.uu.se/OpenSubtitles2016/mono/OpenSubtitles2016.raw.ru.gz'  # noqa
SUBTITLES_RU_URL = 'https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/ru.txt.gz'  # noqa

LINUX_DIR = 'linux'
LINUX_CLONE = 'git://github.com/BurntSushi/linux'

# Grep takes locale settings from the environment. There is a *substantial*
# performance impact for enabling Unicode, so we need to handle this explicitly
# in our benchmarks.
GREP_ASCII = {'LC_ALL': 'C'}
GREP_UNICODE = {'LC_ALL': 'en_US.UTF-8'}

# Sift tries really hard to search everything by default. In our code search
# benchmarks, we don't want that.
SIFT = [
    'sift',
    '--binary-skip',
    '--exclude-files', '.*',
    '--exclude-files', '*.pdf',
]

def bench_subtitles_en_literal1(suite_dir, bins_list):
    '''
    Benchmark the speed of an ASCII string literal.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)}", [binary, pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)


def bench_subtitles_en_literal2(suite_dir, bins_list):
    '''
    Benchmark the speed of an ASCII string literal.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (no mmap)", [binary, '--no-mmap', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal3(suite_dir, bins_list):
    '''
    Benchmark the speed of an ASCII string literal.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (lines)", [binary, '-n', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal4(suite_dir, bins_list):
    '''
    Benchmark the speed of an ASCII string literal.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} UTF-16", [binary, pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal5(suite_dir, bins_list):
    '''
    Benchmark the speed of an ASCII string literal.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (no mmap) UTF-16", [binary, '--no-mmap', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal6(suite_dir, bins_list):
    '''
    Benchmark the speed of an ASCII string literal.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (lines) UTF-16", [binary, '-n', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal_casei1(suite_dir, bins_list):
    '''
    Benchmark the speed of a Unicode-y string case insensitively.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)}", [binary, '-i', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal_casei2(suite_dir, bins_list):
    '''
    Benchmark the speed of a Unicode-y string case insensitively.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)}", [binary, '-n', '-i', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal_casei3(suite_dir, bins_list):
    '''
    Benchmark the speed of a Unicode-y string case insensitively.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} UTF-16", [binary, '-i', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal_casei4(suite_dir, bins_list):
    '''
    Benchmark the speed of a Unicode-y string case insensitively.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} UTF-16", [binary, '-n', '-i', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal_word1(suite_dir, bins_list):
    '''
    Benchmark the speed of finding a literal inside word boundaries.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'(?-u:\b)Sherlock Holmes(?-u:\b)'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (ASCII)", [binary, '-n', pat, en,]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal_word2(suite_dir, bins_list):
    '''
    Benchmark the speed of finding a literal inside word boundaries.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'(?-u:\b)Sherlock Holmes(?-u:\b)'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (ASCII) UTF-16", [binary, '-n', pat, en16,]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal_word3(suite_dir, bins_list):
    '''
    Benchmark the speed of finding a literal inside word boundaries.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)}", [binary, '-nw', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_literal_word4(suite_dir, bins_list):
    '''
    Benchmark the speed of finding a literal inside word boundaries.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = 'Sherlock Holmes'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} UTF-16", [binary, '-nw', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_alternate1(suite_dir, bins_list):
    '''
    Benchmark the speed of a set of alternate literals.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'Sherlock Holmes|John Watson|Irene Adler|Inspector Lestrade|Professor Moriarty'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (lines)", [binary, '-n', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_alternate2(suite_dir, bins_list):
    '''
    Benchmark the speed of a set of alternate literals.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'Sherlock Holmes|John Watson|Irene Adler|Inspector Lestrade|Professor Moriarty'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)}", [binary, pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_alternate3(suite_dir, bins_list):
    '''
    Benchmark the speed of a set of alternate literals.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'Sherlock Holmes|John Watson|Irene Adler|Inspector Lestrade|Professor Moriarty'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (lines) UTF-16", [binary, '-n', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_alternate4(suite_dir, bins_list):
    '''
    Benchmark the speed of a set of alternate literals.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'Sherlock Holmes|John Watson|Irene Adler|Inspector Lestrade|Professor Moriarty'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} UTF-16", [binary, pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_alternate_casei1(suite_dir, bins_list):
    '''
    Benchmark the speed of a set of alternate literals.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'Sherlock Holmes|John Watson|Irene Adler|Inspector Lestrade|Professor Moriarty'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)}", [binary, '-n', '-i', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_alternate_casei2(suite_dir, bins_list):
    '''
    Benchmark the speed of a set of alternate literals.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'Sherlock Holmes|John Watson|Irene Adler|Inspector Lestrade|Professor Moriarty'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} UTF-16", [binary, '-n', '-i', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_surrounding_words1(suite_dir, bins_list):
    '''
    Benchmark a more complex regex with an inner literal.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'\w+\s+Holmes\s+\w+'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)}", [binary, '-n', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_surrounding_words2(suite_dir, bins_list):
    '''
    Benchmark a more complex regex with an inner literal.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'(?-u)\w+\s+Holmes\s+\w+'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (ASCII)", [binary, '-n', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_surrounding_words3(suite_dir, bins_list):
    '''
    Benchmark a more complex regex with an inner literal.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'\w+\s+Holmes\s+\w+'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} UTF-16", [binary, '-n', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_surrounding_words4(suite_dir, bins_list):
    '''
    Benchmark a more complex regex with an inner literal.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'(?-u)\w+\s+Holmes\s+\w+'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (ASCII) UTF-16", [binary, '-n', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_no_literal1(suite_dir, bins_list):
    '''
    Benchmark the speed of a regex with no literals.

    Note that we don't even try to run grep with Unicode support
    on this one. While it should eventually get the right answer,
    I killed it after it had already been running for two minutes
    and showed no signs of finishing soon.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)}", [binary, '-n', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_no_literal2(suite_dir, bins_list):
    '''
    Benchmark the speed of a regex with no literals.

    Note that we don't even try to run grep with Unicode support
    on this one. While it should eventually get the right answer,
    I killed it after it had already been running for two minutes
    and showed no signs of finishing soon.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'(?-u)\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (ASCII)", [binary, '-n', pat, en]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_no_literal3(suite_dir, bins_list):
    '''
    Benchmark the speed of a regex with no literals.

    Note that we don't even try to run grep with Unicode support
    on this one. While it should eventually get the right answer,
    I killed it after it had already been running for two minutes
    and showed no signs of finishing soon.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} UTF-16", [binary, '-n', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)

def bench_subtitles_en_no_literal4(suite_dir, bins_list):
    '''
    Benchmark the speed of a regex with no literals.

    Note that we don't even try to run grep with Unicode support
    on this one. While it should eventually get the right answer,
    I killed it after it had already been running for two minutes
    and showed no signs of finishing soon.
    '''
    require(suite_dir, 'subtitles-en')
    en = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME)
    en16 = path.join(suite_dir, SUBTITLES_DIR, SUBTITLES_EN_NAME16)
    pat = r'(?-u)\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}\s+\w{4}'

    commands_list = []
    for binary in bins_list:
        commands_list.append(Command(f"{os.path.basename(binary)} (ASCII) UTF-16", [binary, '-n', pat, en16]))

    return Benchmark(pattern=pat, commands=commands_list)


class MissingDependencies(Exception):
    '''
    A missing dependency exception.

    This exception occurs when running a benchmark that requires a
    particular corpus that isn't available.

    :ivar list(str) missing_names:
        A list of missing dependency names. These names correspond to
        names that can be used with the --download flag.
    '''
    def __init__(self, missing_names):
        self.missing_names = missing_names

    def __str__(self):
        return 'MissingDependency(%s)' % repr(self.missing_names)


class MissingCommands(Exception):
    '''
    A missing command exception.

    This exception occurs when running a command in a benchmark
    where the command could not be found on the current system.

    :ivar list(str) missing_names:
        The names of the command binaries that could not be found.
    '''
    def __init__(self, missing_names):
        self.missing_names = sorted(set(missing_names))

    def __str__(self):
        return 'MissingCommands(%s)' % repr(self.missing_names)


class Benchmark(object):
    '''
    A single benchmark corresponding to a grouping of commands.

    The main purpose of a benchmark is to compare the performance
    characteristics of a group of commands.
    '''

    def __init__(self, name=None, pattern=None, commands=None,
                 warmup_count=0, count=2, line_count=True,
                 allow_missing_commands=False,
                 disabled_cmds=None, order=0):
        '''
        Create a single benchmark.

        A single benchmark is composed of a set of commands that are
        benchmarked and compared against one another. A benchmark may
        have multiple commands that use the same search tool (but
        probably should have something differentiating them).

        The grouping of commands is a purely human driven process.

        By default, the output of every command is sent to /dev/null.
        Other types of behavior are available via the methods defined
        on this benchmark.

        :param str name:
            A human readable string denoting the name of this
            benchmark.
        :param str pattern:
            The pattern that is used in search.
        :param list(Command) commands:
            A list of commands to initialize this benchmark with. More
            commands may be added before running the benchmark.
        :param int warmup_count:
            The number of times to run each command before recording
            samples.
        :param int count:
            The number of samples to collect from each command.
        :param bool line_count:
            When set, the lines of each search are counted and included
            in the samples produced.
        :param bool allow_missing_commands:
            When set, if a command is missing, then the benchmark
            will simply skip it.
        :param list(str) disabled_cmds:
            A list of commands to skip.
        :param int order:
            An integer indicating the sequence number of this benchmark.
        '''
        self.name = name
        self.pattern = pattern
        self.commands = commands or []
        self.warmup_count = warmup_count
        self.count = 2
        self.line_count = line_count
        self.allow_missing_commands = allow_missing_commands
        self.disabled_cmds = set(disabled_cmds or [])
        self.order = order

    def raise_if_missing(self):
        '''
        Raises a MissingCommands exception if applicable.

        A MissingCommands exception is raised when the following
        criteria are met: 1) allow_missing_commands is False, and 2) at
        least one command in this benchmark could not be found on this
        system.
        '''
        missing_commands = []
        for c in self.commands:
            if c.binary_name in self.disabled_cmds or c.exists():
                continue
            missing_commands.append(c.binary_name)
        if not self.allow_missing_commands and len(missing_commands) > 0:
            raise MissingCommands(missing_commands)

    def run(self):
        '''
        Runs this benchmark and returns the results.

        :rtype: Result
        :raises:
            MissingCommands if any command doesn't exist.
            (Unless allow_missing_commands is enabled.)
        '''
        self.raise_if_missing()
        result = Result(self)
        for cmd in self.commands:
            if cmd.binary_name in self.disabled_cmds:
                continue
            if self.allow_missing_commands and not cmd.exists():
                # Skip this command if we're OK with it.
                continue
            # Do a warmup first.
            for _ in range(self.warmup_count):
                self.run_one(cmd)
            for _ in range(self.count):
                result.add(cmd, **self.run_one(cmd))
        return result

    def run_one(self, cmd):
        '''
        Runs the given command exactly once.

        Returns an object that includes the time taken by the command.
        If this benchmark was configured to count the number of lines
        returned, then the line count is also returned.

        :param Command cmd: The command to run.
        :returns:
            A dict with two fields, duration and line_count.
            The duration is in seconds, with fractional milliseconds,
            and is guaranteed to be available. The line_count is set
            to None unless line counting is enabled, in which case,
            it is the number of lines in the search output.
        :rtype: int
        '''
        if not cmd.exists():
            raise MissingCommands([cmd.cmd[0]])
        cmd.kwargs['stderr'] = subprocess.DEVNULL
        if self.line_count:
            cmd.kwargs['stdout'] = subprocess.PIPE
        else:
            cmd.kwargs['stdout'] = subprocess.DEVNULL

        start = time.time()
        completed = cmd.run()
        end = time.time()

        line_count = None
        if self.line_count:
            line_count = completed.stdout.count(b'\n')
        return {
            'duration': end - start,
            'line_count': line_count,
        }


class Result(object):
    '''
    The result of running a benchmark.

    Benchmark results consist of a set of samples, where each sample
    corresponds to a single run of a single command in the benchmark.
    Various statistics can be computed from these samples such as mean
    and standard deviation.
    '''
    def __init__(self, benchmark):
        '''
        Create a new set of results, initially empty.

        :param Benchmark benchmark:
            The benchmark that produced these results.
        '''
        self.benchmark = benchmark
        self.samples = []

    def add(self, cmd, duration, line_count=None):
        '''
        Add a new sample to this result set.

        :param Command cmd:
            The command that produced this sample.
        :param int duration:
            The duration, in milliseconds, that the command took to
            run.
        :param int line_count:
            The number of lines in the search output. This is optional.
        '''
        self.samples.append({
            'cmd': cmd,
            'duration': duration,
            'line_count': line_count,
        })

    def fastest_sample(self):
        '''
        Returns the fastest recorded sample.
        '''
        return min(self.samples, key=lambda s: s['duration'])

    def fastest_cmd(self):
        '''
        Returns the fastest command according to distribution.
        '''
        means = []
        for cmd in self.benchmark.commands:
            mean, _ = self.distribution_for(cmd)
            if mean is None:
                continue
            means.append((cmd, mean))
        return min(means, key=lambda tup: tup[1])[0]

    def samples_for(self, cmd):
        'Returns an iterable of samples for cmd'
        yield from (s for s in self.samples if s['cmd'].name == cmd.name)

    def line_counts_for(self, cmd):
        '''
        Returns the line counts recorded for each command.

        :returns:
            A dictionary from command name to a set of line
            counts recorded.
        '''
        return {s['line_count']
                for s in self.samples_for(cmd)
                if s['line_count'] is not None}

    def distribution_for(self, cmd):
        '''
        Returns the distribution (mean +/- std) of the given command.

        If there are no samples for this command (i.e., it was skipped),
        then return ``(None, None)``.

        :rtype: (float, float)
        :returns:
            A tuple containing the mean and standard deviation, in that
            order.
        '''
        samples = list(s['duration'] for s in self.samples_for(cmd))
        if len(samples) == 0:
            return None, None
        return statistics.mean(samples), statistics.stdev(samples)


class Command(object):
    def __init__(self, name, cmd, *args, **kwargs):
        '''
        Create a new command that is run as part of a benchmark.

        *args and **kwargs are passed directly to ``subprocess.run``.
        An exception to this is stdin/stdout/stderr. Output
        redirection is completely controlled by the benchmark harness.
        Trying to set them here will trigger an assert.

        :param str name:
            The human readable name of this command. This is
            particularly useful if the same search tool is used
            multiple times in the same benchmark with different
            arguments.
        :param list(str) cmd:
            The command to run as a list of arguments (including the
            command name itself).
        '''
        assert 'stdin' not in kwargs
        assert 'stdout' not in kwargs
        assert 'stderr' not in kwargs
        self.name = name
        self.cmd = cmd
        self.args = args
        self.kwargs = kwargs

    def exists(self):
        'Returns true if and only if this command exists.'
        return shutil.which(self.binary_name) is not None

    @property
    def binary_name(self):
        'Return the binary name of this command.'
        return self.cmd[0]

    def run(self):
        '''
        Runs this command and returns its status.

        :rtype: subprocess.CompletedProcess
        '''
        return subprocess.run(self.cmd, *self.args, **self.kwargs)


def eprint(*args, **kwargs):
    'Like print, but to stderr.'
    kwargs['file'] = sys.stderr
    print(*args, **kwargs)


def run_cmd(cmd, *args, **kwargs):
    '''
    Print the command to stderr and run it.

    If the command fails, throw a traceback.
    '''
    eprint('# %s' % ' '.join(cmd))
    kwargs['check'] = True
    return subprocess.run(cmd, *args, **kwargs)


def require(suite_dir, *names):
    '''
    Declare a dependency on the given names for a benchmark.

    If any dependency doesn't exist, then fail with an error message.
    '''
    errs = []
    for name in names:
        fun_name = name.replace('-', '_')
        if not globals()['has_%s' % fun_name](suite_dir):
            errs.append(name)
    if len(errs) > 0:
        raise MissingDependencies(errs)


def download_linux(suite_dir):
    'Download and build the Linux kernel.'
    checkout_dir = path.join(suite_dir, LINUX_DIR)
    if not os.path.isdir(checkout_dir):
        # Clone from my fork so that we always get the same corpus *and* still
        # do a shallow clone. Shallow clones are much much cheaper than full
        # clones.
        run_cmd(['git', 'clone', '--depth', '1', LINUX_CLONE, checkout_dir])
    # We want to build the kernel because the process of building it produces
    # a lot of junk in the repository that a search tool probably shouldn't
    # touch.
    if not os.path.exists(path.join(checkout_dir, 'vmlinux')):
        eprint('# Building Linux kernel...')
        run_cmd(['make', 'defconfig'], cwd=checkout_dir)
        run_cmd(['make', '-j', str(cpu_count())], cwd=checkout_dir)


def has_linux(suite_dir):
    'Returns true if we believe the Linux kernel is built.'
    checkout_dir = path.join(suite_dir, LINUX_DIR)
    return path.exists(path.join(checkout_dir, 'vmlinux'))


def download_subtitles_en(suite_dir):
    'Download and decompress English subtitles.'
    subtitle_dir = path.join(suite_dir, SUBTITLES_DIR)
    en_path_gz = path.join(subtitle_dir, SUBTITLES_EN_NAME_GZ)
    en_path = path.join(subtitle_dir, SUBTITLES_EN_NAME)
    en_path16 = path.join(subtitle_dir, SUBTITLES_EN_NAME16)
    en_path_sample = path.join(subtitle_dir, SUBTITLES_EN_NAME_SAMPLE)

    if not os.path.isdir(subtitle_dir):
        os.makedirs(subtitle_dir)
    if not os.path.exists(en_path):
        #if not os.path.exists(en_path_gz):
            #run_cmd(['curl', '-LO', SUBTITLES_EN_URL], cwd=subtitle_dir)
        extract_cmd = "pigz -dc en.txt.gz > en.txt"
        cwd = subtitle_dir
        try:
            process = subprocess.run(
                extract_cmd,
                check=True,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                universal_newlines=True,
                cwd=cwd,
            )
        except subprocess.CalledProcessError as err:
            print(f"Unable extract {extract_cmd}: {err}")
            sys.exit(1)
    if not os.path.exists(en_path16):
        #if not os.path.exists(en_path_gz):
            #run_cmd(['curl', '-LO', SUBTITLES_EN_URL], cwd=subtitle_dir)
        extract_cmd = "pigz -dc en16.txt.gz > en16.txt"
        cwd = subtitle_dir
        try:
            process = subprocess.run(
                extract_cmd,
                check=True,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                universal_newlines=True,
                cwd=cwd,
            )
        except subprocess.CalledProcessError as err:
            print(f"Unable extract {extract_cmd}: {err}")
            sys.exit(1)
    #if not os.path.exists(en_path_sample):
        ## Get a sample roughly the same size as the Russian corpus so that
        ## benchmarks finish in a reasonable time.
        #with open(path.join(subtitle_dir, en_path_sample), 'wb+') as f:
            #run_cmd(
                #['head', '-n', '10416550', en_path],
                #cwd=subtitle_dir, stdout=f)


def has_subtitles_en(suite_dir):
    'Returns true if English subtitles have been downloaded.'
    subtitle_dir = path.join(suite_dir, SUBTITLES_DIR)
    return path.exists(path.join(subtitle_dir, SUBTITLES_EN_NAME))


def download_subtitles_ru(suite_dir):
    'Download and decompress Russian subtitles.'
    subtitle_dir = path.join(suite_dir, SUBTITLES_DIR)
    ru_path_gz = path.join(subtitle_dir, SUBTITLES_RU_NAME_GZ)
    ru_path = path.join(subtitle_dir, SUBTITLES_RU_NAME)

    if not os.path.isdir(subtitle_dir):
        os.makedirs(subtitle_dir)
    if not os.path.exists(ru_path):
        if not os.path.exists(ru_path_gz):
            run_cmd(['curl', '-LO', SUBTITLES_RU_URL], cwd=subtitle_dir)
        run_cmd(['gunzip', ru_path_gz])


def has_subtitles_ru(suite_dir):
    'Returns true if Russian subtitles have been downloaded.'
    subtitle_dir = path.join(suite_dir, SUBTITLES_DIR)
    return path.exists(path.join(subtitle_dir, SUBTITLES_RU_NAME))


def download(suite_dir, choices):
    '''
    Download choices into suite_dir.

    Specifically, choices specifies a list of corpora to fetch.

    :param str suite_dir:
        The directory in which to download corpora.
    :param list(str) choices:
        A list of corpora to download. Available choices are:
        all, linux, subtitles-en, subtitles-ru.
    '''
    for choice in choices:
        if choice == 'linux':
            download_linux(suite_dir)
        elif choice == 'subtitles-en':
            download_subtitles_en(suite_dir)
        elif choice == 'subtitles-ru':
            download_subtitles_ru(suite_dir)
        elif choice == 'all':
            download_linux(suite_dir)
            download_subtitles_en(suite_dir)
            download_subtitles_ru(suite_dir)
        else:
            eprint('Unrecognized download choice: %s' % choice)
            sys.exit(1)


def collect_benchmarks(suite_dir, bins_list, filter_pat=None,
                       allow_missing_commands=False,
                       disabled_cmds=None,
                       warmup_iter=1, bench_iter=3):
    '''
    Return an iterable of all runnable benchmarks.

    :param str suite_dir:
        The directory containing corpora.
    :param str filter_pat:
        A single regular expression that is used to filter benchmarks
        by their name. When not specified, all benchmarks are run.
    :returns:
        An iterable over all runnable benchmarks. If a benchmark
        requires corpora that are missing, then a log message is
        emitted to stderr and it is not yielded.
    '''
    benchmarks = []
    for global_name in globals():
        if not global_name.startswith('bench_'):
            continue
        name = re.sub('^bench_', '', global_name)
        if filter_pat is not None and not re.search(filter_pat, name):
            continue
        try:
            fun = globals()[global_name]
            benchmark = fun(suite_dir, bins_list)
            benchmark.name = name
            benchmark.warmup_count = warmup_iter
            benchmark.count = bench_iter
            benchmark.allow_missing_commands = allow_missing_commands
            benchmark.disabled_cmds = disabled_cmds
            benchmark.order = fun.__code__.co_firstlineno
            benchmark.raise_if_missing()
        except MissingDependencies as e:
            eprint(
                'missing: %s, skipping benchmark %s (try running with: %s)' % (
                    ', '.join(e.missing_names),
                    name,
                    ' '.join(['--download %s' % n for n in e.missing_names]),
                ))
            continue
        except MissingCommands as e:
            fmt = 'missing commands: %s, skipping benchmark %s ' \
                  '(run with --allow-missing to run incomplete benchmarks)'
            eprint(fmt % (', '.join(e.missing_names), name))
            continue
        benchmarks.append(benchmark)
    return sorted(benchmarks, key=lambda b: b.order)

class BinStatistics(object):
    def __init__(self, name):
        self.basename = name
        self.sum_means = 0.0
        self.sum_std_dev_means = 0.0
        self.bin_wins = []
        self.bin_wins_total = 0
        self.bin_fastest_wins = []
        self.bin_fastest_wins_total = 0
        self.wins_total_print = ""
        self.fastest_wins_total_print = ""
        self.sum_means_print = ""
        self.np_sum_means_print = ""
        self.all_results = []

class Bin(object):
    def __init__(self, fullpath):
        self.basename = os.path.basename(fullpath)
        self.fullpath = fullpath
        self.sum_means = 0.0
        self.sum_std_dev_means = 0.0
        self.bin_wins = []
        self.bin_wins_total = 0
        self.bin_fastest_wins = []
        self.bin_fastest_wins_total = 0
        self.wins_total_print = ""
        self.fastest_wins_total_print = ""
        self.sum_means_print = ""
        self.np_sum_means_print = ""
        self.all_results = []
        self.sha256 = get_checksum(fullpath, algorithm="SHA256")

    def __repr__(self):
        return repr((self.basename, self.sum_means, self.sum_std_dev_means,
                     self.bin_wins_total, self.bin_fastest_wins_total))

def write_out(filename, content, mode="w"):
    """File.write convenience wrapper."""
    with open_auto(filename, mode) as require_f:
        require_f.write(content)


def open_auto(*args, **kwargs):
    """Open a file with UTF-8 encoding.

    Open file with UTF-8 encoding and "surrogate" escape characters that are
    not valid UTF-8 to avoid data corruption.
    """
    # 'encoding' and 'errors' are fourth and fifth positional arguments, so
    # restrict the args tuple to (file, mode, buffering) at most
    assert len(args) <= 3
    assert 'encoding' not in kwargs
    assert 'errors' not in kwargs
    return open(*args, encoding="utf-8", errors="surrogateescape", **kwargs)

def main():
    script_dirname = f"{os.path.abspath(os.path.dirname( __file__ ))}"

    download_choices = ['all', 'linux', 'subtitles-en', 'subtitles-ru']
    p = argparse.ArgumentParser('Command line search tool benchmark suite.')
    p.add_argument(
        '--dir', metavar='PATH', default=script_dirname,
        help='The directory in which to download data and perform searches.')
    p.add_argument(
        '--download', metavar='CORPUS', action='append',
        choices=download_choices,
        help='Download and prepare corpus data, then exit without running '
             'any benchmarks. Note that this command is intended to be '
             'idempotent. WARNING: This downloads over a gigabyte of data, '
             'and also includes building the Linux kernel. If "all" is used '
             'then the total uncompressed size is around 13 GB. '
             'Choices: %s' % ', '.join(download_choices))
    p.add_argument(
        '--allow-missing', action='store_true', default=True,
        help='Permit benchmarks to run even if some commands are missing.')
    p.add_argument(
        '--disabled', help='A list of comma separated commands to skip.')
    p.add_argument(
        '-f', '--force', action='store_true', default=True,
        help='Overwrite existing files if there is a conflict.')
    p.add_argument(
        '--list', action='store_true',
        help='List available benchmarks by name.')
    p.add_argument(
        '--raw', metavar='PATH', default=f"{script_dirname}/raw.csv",
        help='Dump raw data (all samples collected) in CSV format to the '
             'file path provided.')
    p.add_argument(
        '--warmup-iter', metavar='INTEGER', type=int, default=1,
        help='The number of iterations to run each command before '
             'recording measurements.')
    p.add_argument(
        '--bench-iter', metavar='INTEGER', type=int, default=3,
        help='The number of iterations to run each command while '
             'recording measurements.')
    p.add_argument(
        'bench', metavar='PAT', nargs='?',
        help='A regex pattern that will only run benchmarks that match.')
    p.add_argument(
        '--bins', default="", help='Binaries to be tested: --bins PATH1,PATH2,PATH3.')
    p.add_argument(
        '--ttest', action='store_true', help="Perform Welch's t-test on two set of results written on raw.csv")
    p.add_argument(
        '--flags', default="", help='Flags')
    p.add_argument(
        '--bolt-flags', default="", help='Bolt flags')
    args = p.parse_args()


    if args.ttest:
        binaries = []
        bins_added = []
        with open(f"{os.path.abspath(os.path.dirname( __file__ ))}/raw.csv", 'r+') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                name = row['name'].split(' ')[0]
                if name not in bins_added:
                    bins_added.append(name)
                    binaries.append(BinStatistics(name))
                for binary in binaries:
                    if binary.basename == name:
                        binary.all_results.append(row['duration'])


        total_bins = len(binaries)
        for binary in binaries:
            binary.np_all_results = np.array(binary.all_results, dtype=np.float64)
            binary.np_mean = np.mean(binary.np_all_results, dtype = np.float64)
            binary.np_stddev = np.std(binary.np_all_results, dtype = np.float64)
            binary.np_var = np.var(binary.np_all_results, dtype = np.float64)
            #binary.np_stddev = np.std(binary.np_all_results, ddof=1, dtype = np.float64)
            #binary.np_var = np.var(binary.np_all_results, ddof=1, dtype = np.float64)
            binary.bayes_mean, binary.bayes_var, binary.bayes_stddev = stats.bayes_mvs(binary.np_all_results, alpha=0.95)
            #print(f"{binary.basename} binary.np_all_results:\n{binary.np_all_results}")
            #print(f"{binary.basename} binary.np_mean: {binary.np_mean}")
            #print(f"{binary.basename} binary.np_stddev: {binary.np_stddev}")
            #print(f"{binary.basename} binary.np_var: {binary.np_var}\n")

        pd_all_results = pd.DataFrame(data={'milliseconds': np.concatenate((binaries[0].np_all_results, binaries[1].np_all_results),  dtype = np.float64),
            'binary': [binaries[0].basename for x in range(len(binaries[0].np_all_results))] + [binaries[1].basename for x in range(len(binaries[1].np_all_results))]})

        binaries.sort(key=lambda binary: binary.bayes_mean.statistic, reverse=False)
        for i in range(total_bins):
            if total_bins > 1:
                if (i == 0):
                    if (binaries[i].bayes_mean.statistic < binaries[i+1].bayes_mean.statistic):
                        diff_total = 0.0
                        diff_total = binaries[i+1].bayes_mean.statistic - binaries[i].bayes_mean.statistic
                        diff_percent = (diff_total*100)/binaries[i+1].bayes_mean.statistic
                        binaries[i].np_sum_means_print = f"\033[31;1m{binaries[i].basename} total np\nM={binaries[i].bayes_mean.statistic:0.5f}, 95% CI [{binaries[i].bayes_mean.minmax[0]:0.5f}, {binaries[i].bayes_mean.minmax[1]:0.5f}] -{diff_total:0.5f}ms or +{diff_percent:0.5f}% faster than the second\nSD={binaries[i].bayes_stddev.statistic:0.5f}, 95% CI [{binaries[i].bayes_stddev.minmax[0]:0.5f}, {binaries[i].bayes_stddev.minmax[1]:0.5f}]\nVAR={binaries[i].bayes_var.statistic:0.5f}, 95% CI [{binaries[i].bayes_var.minmax[0]:0.5f}, {binaries[i].bayes_var.minmax[1]:0.5f}]\033[0m"
                    else:
                        binaries[i].np_sum_means_print = f"\033[34;1m{binaries[i].basename} total np\nM={binaries[i].bayes_mean.statistic:0.5f}, 95% CI [{binaries[i].bayes_mean.minmax[0]:0.5f}, {binaries[i].bayes_mean.minmax[1]:0.5f}]\nSD={binaries[i].bayes_stddev.statistic:0.5f}, 95% CI [{binaries[i].bayes_stddev.minmax[0]:0.5f}, {binaries[i].bayes_stddev.minmax[1]:0.5f}]\nVAR={binaries[i].bayes_var.statistic:0.5f}, 95% CI [{binaries[i].bayes_var.minmax[0]:0.5f}, {binaries[i].bayes_var.minmax[1]:0.5f}]\033[0m"
                else:
                    binaries[i].np_sum_means_print = f"{binaries[i].basename} total np\nM={binaries[i].bayes_mean.statistic:0.5f}, 95% CI [{binaries[i].bayes_mean.minmax[0]:0.5f}, {binaries[i].bayes_mean.minmax[1]:0.5f}]\nSD={binaries[i].bayes_stddev.statistic:0.5f}, 95% CI [{binaries[i].bayes_stddev.minmax[0]:0.5f}, {binaries[i].bayes_stddev.minmax[1]:0.5f}]\nVAR={binaries[i].bayes_var.statistic:0.5f}, 95% CI [{binaries[i].bayes_var.minmax[0]:0.5f}, {binaries[i].bayes_var.minmax[1]:0.5f}]"
            else:
                binaries[i].np_sum_means_print = f"{binaries[i].basename} total np\nM={binaries[i].bayes_mean.statistic:0.5f}, 95% CI [{binaries[i].bayes_mean.minmax[0]:0.5f}, {binaries[i].bayes_mean.minmax[1]:0.5f}]\nSD={binaries[i].bayes_stddev.statistic:0.5f}, 95% CI [{binaries[i].bayes_stddev.minmax[0]:0.5f}, {binaries[i].bayes_stddev.minmax[1]:0.5f}]\nVAR={binaries[i].bayes_var.statistic:0.5f}, 95% CI [{binaries[i].bayes_var.minmax[0]:0.5f}, {binaries[i].bayes_var.minmax[1]:0.5f}]"

        for binary in binaries:
            print("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
            print(f"{binary.np_sum_means_print}\n")
            #print(f"{binary.bayes_mean}")
            #print(f"{binary.bayes_stddev}")
            #print(f"{binary.bayes_var}\n")

        print("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
        t, p = stats.ttest_ind(binaries[0].np_all_results, binaries[1].np_all_results, alternative="two-sided", equal_var=False)
        th = 0.05
        dispose = p < th
        print("t = {:.3}, p = {:.3}".format(t, p))
        print()
        if dispose:
            print("\033[31;1mSignificant difference between the two benchmarks (p < {})\033[0m\n".format(th))
        else:
            print("\033[34;1mThe two benchmarks are almost the same (p >= {})\033[0m\n".format(th))

        #sns.set_theme()
        #sns.set(rc={"figure.dpi":200, 'savefig.dpi':200})
        sns.set_style("darkgrid", {"axes.facecolor": ".9"})
        sns.set_context("notebook", rc={"grid.linewidth": 0.4, "figure.dpi" : 200, "savefig.dpi" : 200})
        fig = plt.figure(figsize=(3560/200, 1212/200), dpi=200)
        ax1 = fig.add_subplot(111)
        fig.set_dpi(200)

        #ax1 = sns.histplot(ax=ax1, data=pd_all_results, x="milliseconds", hue="binary", element="step", stat="probability", line_kws={'linewidth': 0.5}, kde=True, bins=pd_all_results["milliseconds"].count())
        sns.histplot(ax=ax1, data=pd_all_results, x="milliseconds", hue="binary", stat="count", line_kws={'linewidth': 0.4}, linewidth=0.4, kde=True, element="step", bins="fd")

        ax1.set_ylabel("count", fontsize= 6)
        ax1.set_xlabel("milliseconds", fontsize= 6)
        plt.title("benchsuite", fontsize= 6)
        plt.xticks(fontsize=5)
        plt.yticks(fontsize=5)

        arr = (seg[0][1] for seg in ax1.collections[0].get_paths()[0].iter_segments())
        #y_values = np.array(tuple(arr), dtype = np.float64)
        histplot_y_max = np.amax(np.array(tuple(arr), dtype = np.float64))
        #print(f"histplot_y_max: {histplot_y_max:0.5f}")

        ax1.axvline(x=binaries[0].bayes_mean.statistic, ymin=0, ymax=1, color='red', label=f"{binaries[0].basename} mean", linewidth=0.3, linestyle='--', zorder=5)
        ax1.axvline(x=binaries[1].bayes_mean.statistic, ymin=0, ymax=1, color='blue', label=f"{binaries[1].basename} mean", linewidth=0.3, linestyle='--', zorder=5)

        handles1, labels1 = ax1.get_legend_handles_labels()
        legend = ax1.get_legend()
        handles2 = legend.legendHandles
        #print(handles1)
        #print(labels1)
        #print(handles2)
        handles2 += handles1
        legend.remove()
        ax1.legend(handles2, [f"{binaries[0].basename}", f"{binaries[1].basename}", f"{binaries[0].basename} mean", f"{binaries[1].basename} mean"])

        ax1.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))
        ax1.set_xticks(np.arange(0, pd_all_results["milliseconds"].max()+0.01, 0.01))
        ax1.set_yticks(np.arange(0, histplot_y_max+5, 5))
        ax1.set(xlim=(-0.01, np.around(pd_all_results["milliseconds"].max()+0.01, 2)), ylim=(0,histplot_y_max+5))
        #print(f"ax1.get_xlim(): {ax1.get_xlim()}")
        #print(f"ax1.get_ylim(): {ax1.get_ylim()}")
        #print(f"ax1.get_xticks(): {ax1.get_xticks()}")
        #print(f"ax1.get_yticks(): {ax1.get_yticks()}")


        mean_zoom = np.array(0.0, dtype = np.float64)
        if binaries[0].bayes_mean.statistic > binaries[1].bayes_mean.statistic:
            mean_zoom = np.array(binaries[1].bayes_mean.statistic, dtype = np.float64)
        else:
            mean_zoom = np.array(binaries[0].bayes_mean.statistic, dtype = np.float64)

        ax2 = plt.axes([mean_zoom*5.55, 0.76, .1, .1], facecolor='khaki')
        for spine in ax2.spines.values():
            spine.set_edgecolor('black')
            spine.set_linewidth(0.7)
        ax2.grid(True, color ="black", alpha=0.3)

        ax2.axvline(x=binaries[0].bayes_mean.statistic, ymin=0, ymax=1, color='red', linewidth=0.4, linestyle='--', zorder=5)
        ax2.axvline(x=binaries[1].bayes_mean.statistic, ymin=0, ymax=1, color='blue', linewidth=0.4, linestyle='--', zorder=5)

        ax2.xaxis.set_major_formatter(FormatStrFormatter('%.4f'))
        ax2.set_xticks(np.arange(binaries[0].bayes_mean.statistic-0.001, binaries[0].bayes_mean.statistic+0.001, 0.0005))
        ax2.set(xlim=(binaries[0].bayes_mean.statistic-0.001, binaries[0].bayes_mean.statistic+0.001), ylim=(ax1.get_ylim()[1]-20, ax1.get_ylim()[1]-10))
        ax2.xaxis.set_ticks_position('top')
        ax2.tick_params(pad=0.1, width=0.5, length=2.5, direction='out')
        plt.setp(ax2.get_yticklabels(), visible=False)
        plt.xticks(fontsize=4)
        plt.yticks(fontsize=4)

        #mark_inset(ax1, ax2, loc1=3, loc2=2, fc="none", ec="black", linewidth=0.4, zorder=2)
        mark_inset(ax1, ax2, loc1=4, loc2=2, fc="none", ec="black", linewidth=0.3, zorder=1)

        ax1.autoscale_view()
        plt.tight_layout(pad=0.1)
        fig.subplots_adjust(left=0.020)
        #print(sns.axes_style())
        plt.show()
        sys.exit(0)


    if len(args.bins) > 0:
        bins_list = args.bins.split(',')
        binaries = [Bin(x) for x in bins_list]
    else:
        print("Need at least two binaries for benchmarking comparison")
        sys.exit(1)

    output_buffer = []



    if args.list:
        benchmarks = collect_benchmarks(
            args.dir, bins_list, filter_pat=args.bench,
            allow_missing_commands=args.allow_missing,
            disabled_cmds=(args.disabled or '').split(','),
            warmup_iter=args.warmup_iter, bench_iter=args.bench_iter)
        for b in benchmarks:
            print(b.name)
        sys.exit(0)
    if args.download is not None and len(args.download) > 0:
        download(args.dir, args.download)
        sys.exit(0)

    if not path.isdir(args.dir):
        os.makedirs(args.dir)
    if args.raw is not None and path.exists(args.raw) and not args.force:
        eprint('File %s already exists (delete it or use --force)' % args.raw)
        sys.exit(1)
    raw_handle, raw_csv_wtr = None, None
    if args.raw is not None:
        fields = [
            'benchmark', 'warmup_iter', 'iter',
            'name', 'command', 'duration', 'lines', 'env',
        ]
        raw_handle = open(args.raw, 'w+')
        raw_csv_wtr = csv.DictWriter(raw_handle, fields)
        raw_csv_wtr.writerow({x: x for x in fields})

    benchmarks = collect_benchmarks(
        args.dir, bins_list, filter_pat=args.bench,
        allow_missing_commands=args.allow_missing,
        disabled_cmds=(args.disabled or '').split(','),
        warmup_iter=args.warmup_iter, bench_iter=args.bench_iter)
    for i, b in enumerate(benchmarks):
        result = b.run()
        fastest_cmd = result.fastest_cmd()
        fastest_sample = result.fastest_sample()
        max_name_len = max(len(cmd.name) for cmd in b.commands)


        header = '%s (pattern: %s)' % (b.name, b.pattern)
        header_final = '\n%s\n%s' % (header, '-' * len(header))
        print(header_final)
        output_buffer.append(header_final)
        for cmd in b.commands:
            name = cmd.name
            mean, stdev = result.distribution_for(cmd)
            if mean is None:
                # If we couldn't get a distribution for this command then
                # it was skipped.
                continue
            line_counts = result.line_counts_for(cmd)
            show_fast_cmd, show_line_counts = '', ''
            if fastest_cmd.name == cmd.name:
                show_fast_cmd = '*'
            if fastest_sample['cmd'].name == cmd.name:
                name += '*'
                for binary in binaries:
                    if binary.basename == os.path.basename(cmd.cmd[0]):
                        binary.bin_fastest_wins.append(f"{b.name}")
            if len(line_counts) > 0:
                counts = map(str, line_counts)
                show_line_counts = ' (lines: %s)' % ', '.join(counts)
            for binary in binaries:
                if binary.basename == os.path.basename(cmd.cmd[0]):
                    binary.sum_means = binary.sum_means + mean
                    binary.sum_std_dev_means = binary.sum_std_dev_means + stdev
            fmt = '{name:{pad}} {mean:0.5f} +/- {stdev:0.5f}{lines}{fast_cmd}'
            if fastest_cmd.name == cmd.name:
                fmt = '\033[31;1m{name:{pad}} {mean:0.5f} +/- {stdev:0.5f}{lines}{fast_cmd}\033[0m'
                for binary in binaries:
                    if binary.basename == os.path.basename(cmd.cmd[0]):
                        binary.bin_wins.append(f"{b.name}")
            print_result_final = fmt.format(
                name=name, pad=max_name_len + 2, fast_cmd=show_fast_cmd,
                mean=mean, stdev=stdev, lines=show_line_counts)
            print(print_result_final)
            output_buffer.append(print_result_final)
        sys.stdout.flush()

        if raw_csv_wtr is not None:
            for sample in result.samples:
                cmd, duration = sample['cmd'], sample['duration']
                env = ' '.join(['%s=%s' % (k, v)
                                for k, v in cmd.kwargs.get('env', {}).items()])
                raw_csv_wtr.writerow({
                    'benchmark': b.name,
                    'warmup_iter': b.warmup_count,
                    'iter': b.count,
                    'name': sample['cmd'].name,
                    'command': ' '.join(cmd.cmd),
                    'duration': duration,
                    'lines': sample['line_count'] or '',
                    'env': env,
                })
            raw_handle.flush()

    for binary in binaries:
        binary.bin_wins_total = len(binary.bin_wins)
        binary.bin_fastest_wins_total = len(binary.bin_fastest_wins)
    total_benchmarks = len(benchmarks)
    total_bins = len(binaries)

    #time_to_wait = 5
    #time_counter = 0
    #while not os.path.exists("raw.csv"):
        #time.sleep(1)
        #time_counter += 1
        #if time_counter > time_to_wait:
            #break
    with open(f"{os.path.abspath(os.path.dirname( __file__ ))}/raw.csv", 'r+') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            for binary in binaries:
                if binary.basename in row['name']:
                    binary.all_results.append(row['duration'])

    for binary in binaries:
        binary.np_all_results = np.array(binary.all_results, dtype=np.float64)
        binary.np_mean = np.mean(binary.np_all_results, dtype = np.float64)
        binary.np_stddev = np.std(binary.np_all_results, dtype = np.float64)
        binary.np_var = np.var(binary.np_all_results, dtype = np.float64)
        binary.bayes_mean, binary.bayes_var, binary.bayes_stddev = stats.bayes_mvs(binary.np_all_results, alpha=0.95)

    pd_all_results = pd.DataFrame(data={'milliseconds': np.concatenate((binaries[0].np_all_results, binaries[1].np_all_results),  dtype = np.float64),
          'binary': [binaries[0].basename for x in range(len(binaries[0].np_all_results))] + [binaries[1].basename for x in range(len(binaries[1].np_all_results))]})

    print("\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
    output_buffer.append("\n\n\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")


    binaries.sort(key=lambda binary: binary.bin_wins_total, reverse=True)
    for i in range(total_bins):
        if total_bins > 1:
            if (i == 0):
                if (binaries[i].bin_wins_total != binaries[i+1].bin_wins_total):
                    #print(f"\033[31;1m{binaries[i].basename} total wins: {binaries[i].bin_wins_total}/{total_benchmarks}\033[0m")
                    binaries[i].wins_total_print = f"\033[31;1m{binaries[i].basename} total wins: {binaries[i].bin_wins_total}/{total_benchmarks}\033[0m"
                    #print(f"{' '.join(binaries[i].bin_wins)}\n")
                else:
                    #print(f"\033[34;1m{binaries[i].basename} total wins: {binaries[i].bin_wins_total}/{total_benchmarks}\033[0m")
                    binaries[i].wins_total_print = f"\033[34;1m{binaries[i].basename} total wins: {binaries[i].bin_wins_total}/{total_benchmarks}\033[0m"
                    #print(f"{' '.join(binaries[i].bin_wins)}\n")
            else:
                #print(f"{binaries[i].basename} total wins: {binaries[i].bin_wins_total}/{total_benchmarks}")
                binaries[i].wins_total_print = f"{binaries[i].basename} total wins: {binaries[i].bin_wins_total}/{total_benchmarks}"
                #print(f"{' '.join(binaries[i].bin_wins)}\n")
        else:
            #print(f"\033[31;1m{binaries[i].basename} total wins: {binaries[i].bin_wins_total}/{total_benchmarks}\033[0m")
            binaries[i].wins_total_print = f"\033[31;1m{binaries[i].basename} total wins: {binaries[i].bin_wins_total}/{total_benchmarks}\033[0m"
            #print(f"{' '.join(binaries[i].bin_wins)}\n")

    binaries.sort(key=lambda binary: binary.bin_fastest_wins_total, reverse=True)
    for i in range(total_bins):
        if total_bins > 1:
            if (i == 0):
                if (binaries[i].bin_fastest_wins_total != binaries[i+1].bin_fastest_wins_total):
                    #print(f"\033[31;1m{binaries[i].basename} fastest runs: {binaries[i].bin_fastest_wins_total}/{total_benchmarks}\033[0m")
                    binaries[i].fastest_wins_total_print = f"\033[31;1m{binaries[i].basename} fastest runs: {binaries[i].bin_fastest_wins_total}/{total_benchmarks}\033[0m"
                    #print(f"{' '.join(binaries[i].bin_fastest_wins)}\n")
                else:
                    #print(f"\033[34;1m{binaries[i].basename} fastest runs: {binaries[i].bin_fastest_wins_total}/{total_benchmarks}\033[0m")
                    binaries[i].fastest_wins_total_print = f"\033[34;1m{binaries[i].basename} fastest runs: {binaries[i].bin_fastest_wins_total}/{total_benchmarks}\033[0m"
                    #print(f"{' '.join(binaries[i].bin_fastest_wins)}\n")
            else:
                #print(f"{binaries[i].basename} fastest runs: {binaries[i].bin_fastest_wins_total}/{total_benchmarks}")
                binaries[i].fastest_wins_total_print = f"{binaries[i].basename} fastest runs: {binaries[i].bin_fastest_wins_total}/{total_benchmarks}"
                #print(f"{' '.join(binaries[i].bin_fastest_wins)}\n")
        else:
            #print(f"\033[31;1m{binaries[i].basename} fastest runs: {binaries[i].bin_fastest_wins_total}/{total_benchmarks}\033[0m")
            binaries[i].fastest_wins_total_print = f"\033[31;1m{binaries[i].basename} fastest runs: {binaries[i].bin_fastest_wins_total}/{total_benchmarks}\033[0m"
            #print(f"{' '.join(binaries[i].bin_fastest_wins)}\n")

    binaries.sort(key=lambda binary: binary.sum_means, reverse=False)
    for i in range(total_bins):
        if total_bins > 1:
            if (i == 0):
                if (binaries[i].sum_means != binaries[i+1].sum_means):
                    diff_total = 0.0
                    diff_total = binaries[i+1].sum_means - binaries[i].sum_means
                    diff_percent = (diff_total*100)/binaries[i+1].sum_means
                    #print(f"\n\033[31;1m{binaries[i].basename} total: {binaries[i].sum_means:0.5f} +/- {binaries[i].sum_std_dev_means:0.5f}\033[0m")
                    #print(f"\033[31;1m-{diff_total:0.5f}ms or +{diff_percent:0.5f}% faster than the second\033[0m")
                    binaries[i].sum_means_print = f"\n\033[31;1m{binaries[i].basename} total: {binaries[i].sum_means:0.5f} +/- {binaries[i].sum_std_dev_means:0.5f}\n-{diff_total:0.5f}ms or +{diff_percent:0.5f}% faster than the second\033[0m"
                else:
                    #print(f"\n\033[34;1m{binaries[i].basename} total: {binaries[i].sum_means:0.5f} +/- {binaries[i].sum_std_dev_means:0.5f}\033[0m")
                    binaries[i].sum_means_print = f"\n\033[34;1m{binaries[i].basename} total: {binaries[i].sum_means:0.5f} +/- {binaries[i].sum_std_dev_means:0.5f}\033[0m"
            else:
                #print(f"\n{binaries[i].basename} total: {binaries[i].sum_means:0.5f} +/- {binaries[i].sum_std_dev_means:0.5f}")
                binaries[i].sum_means_print = f"\n{binaries[i].basename} total: {binaries[i].sum_means:0.5f} +/- {binaries[i].sum_std_dev_means:0.5f}"
        else:
            #print(f"\n{binaries[i].basename} total: {binaries[i].sum_means:0.5f} +/- {binaries[i].sum_std_dev_means:0.5f}")
            binaries[i].sum_means_print = f"\n{binaries[i].basename} total: {binaries[i].sum_means:0.5f} +/- {binaries[i].sum_std_dev_means:0.5f}"

    log_winner_data = ""
    binaries.sort(key=lambda binary: binary.bayes_mean.statistic, reverse=False)
    for i in range(total_bins):
        if total_bins > 1:
            if (i == 0):
                if (binaries[i].bayes_mean.statistic < binaries[i+1].bayes_mean.statistic):
                    diff_total = 0.0
                    diff_total = binaries[i+1].bayes_mean.statistic - binaries[i].bayes_mean.statistic
                    diff_percent = (diff_total*100)/binaries[i+1].bayes_mean.statistic
                    binaries[i].np_sum_means_print = f"\033[31;1m{binaries[i].basename} total np\nM={binaries[i].bayes_mean.statistic:0.5f}, 95% CI [{binaries[i].bayes_mean.minmax[0]:0.5f}, {binaries[i].bayes_mean.minmax[1]:0.5f}] -{diff_total:0.5f}ms or +{diff_percent:0.5f}% faster than the second\nSD={binaries[i].bayes_stddev.statistic:0.5f}, 95% CI [{binaries[i].bayes_stddev.minmax[0]:0.5f}, {binaries[i].bayes_stddev.minmax[1]:0.5f}]\nVAR={binaries[i].bayes_var.statistic:0.5f}, 95% CI [{binaries[i].bayes_var.minmax[0]:0.5f}, {binaries[i].bayes_var.minmax[1]:0.5f}]\033[0m"
                    log_winner_data = f"M={binaries[i].bayes_mean.statistic:0.5f}, 95 percent CI [{binaries[i].bayes_mean.minmax[0]:0.5f}, {binaries[i].bayes_mean.minmax[1]:0.5f}] -{diff_total:0.5f}ms or +{diff_percent:0.5f} percent faster than the second"
                else:
                    binaries[i].np_sum_means_print = f"\033[34;1m{binaries[i].basename} total np\nM={binaries[i].bayes_mean.statistic:0.5f}, 95% CI [{binaries[i].bayes_mean.minmax[0]:0.5f}, {binaries[i].bayes_mean.minmax[1]:0.5f}]\nSD={binaries[i].bayes_stddev.statistic:0.5f}, 95% CI [{binaries[i].bayes_stddev.minmax[0]:0.5f}, {binaries[i].bayes_stddev.minmax[1]:0.5f}]\nVAR={binaries[i].bayes_var.statistic:0.5f}, 95% CI [{binaries[i].bayes_var.minmax[0]:0.5f}, {binaries[i].bayes_var.minmax[1]:0.5f}]\033[0m"
            else:
                binaries[i].np_sum_means_print = f"{binaries[i].basename} total np\nM={binaries[i].bayes_mean.statistic:0.5f}, 95% CI [{binaries[i].bayes_mean.minmax[0]:0.5f}, {binaries[i].bayes_mean.minmax[1]:0.5f}]\nSD={binaries[i].bayes_stddev.statistic:0.5f}, 95% CI [{binaries[i].bayes_stddev.minmax[0]:0.5f}, {binaries[i].bayes_stddev.minmax[1]:0.5f}]\nVAR={binaries[i].bayes_var.statistic:0.5f}, 95% CI [{binaries[i].bayes_var.minmax[0]:0.5f}, {binaries[i].bayes_var.minmax[1]:0.5f}]"
        else:
            binaries[i].np_sum_means_print = f"{binaries[i].basename} total np\nM={binaries[i].bayes_mean.statistic:0.5f}, 95% CI [{binaries[i].bayes_mean.minmax[0]:0.5f}, {binaries[i].bayes_mean.minmax[1]:0.5f}]\nSD={binaries[i].bayes_stddev.statistic:0.5f}, 95% CI [{binaries[i].bayes_stddev.minmax[0]:0.5f}, {binaries[i].bayes_stddev.minmax[1]:0.5f}]\nVAR={binaries[i].bayes_var.statistic:0.5f}, 95% CI [{binaries[i].bayes_var.minmax[0]:0.5f}, {binaries[i].bayes_var.minmax[1]:0.5f}]"

    for binary in binaries:
        print(binary.wins_total_print)
        output_buffer.append(binary.wins_total_print)
        print(f"{' '.join(binary.bin_wins)}\n")
        output_buffer.append(f"{' '.join(binary.bin_wins)}\n")
        print(binary.fastest_wins_total_print)
        output_buffer.append(binary.fastest_wins_total_print)
        print(f"{' '.join(binary.bin_fastest_wins)}")
        output_buffer.append(f"{' '.join(binary.bin_fastest_wins)}")
        print(f"{binary.sum_means_print}\n")
        output_buffer.append(f"{binary.sum_means_print}\n")
        #print(f"{binary.np_all_results}")
        print(f"{binary.np_sum_means_print}\n")
        output_buffer.append(f"{binary.np_sum_means_print}\n")
        #print(f"{binary.bayes_mean}\n")
        #print(f"{binary.bayes_stddev}")
        #print(f"{binary.bayes_var}\n")
        print("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
        output_buffer.append("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")

    t, p = stats.ttest_ind(binaries[0].np_all_results, binaries[1].np_all_results, alternative="two-sided", equal_var=False)
    th = 0.05
    dispose = p < th
    print("t = {:.3}, p = {:.3}\n".format(t, p))
    output_buffer.append("t = {:.3}, p = {:.3}\n".format(t, p))
    if dispose:
        print("\033[31;1mSignificant difference between the two benchmarks (p < {})\033[0m\n".format(th))
        output_buffer.append("\033[31;1mSignificant difference between the two benchmarks (p < {})\033[0m\n".format(th))
    else:
        print("\033[34;1mThe two benchmarks are almost the same (p >= {})\033[0m\n".format(th))
        output_buffer.append("\033[34;1mThe two benchmarks are almost the same (p >= {})\033[0m\n".format(th))

    print("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
    output_buffer.append("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
    print(f"\n\nNumber of benchmarks: {total_benchmarks}")
    output_buffer.append(f"\n\nNumber of benchmarks: {total_benchmarks}\n")

    output_buffer_256 = hashlib.new('sha256')
    for p in output_buffer:
        output_buffer_256.update(p.encode())

    output_hash = output_buffer_256.hexdigest()
    figure_path = f"{script_dirname}/{output_hash}.png"

    log_fullpath = f"{script_dirname}/{output_hash}.log"
    log_full = '\n'.join(output_buffer)
    write_out(log_fullpath, log_full)

    config = configparser.ConfigParser()
    data_ini_fullpath = f"{script_dirname}/data.ini"
    new_raw_fullpath = f"{script_dirname}/{output_hash}.csv"
    config.read(data_ini_fullpath)
    config[output_hash] = {}
    config[output_hash]['flags'] = args.flags
    config[output_hash]['bolt_flags'] = args.bolt_flags
    config[output_hash]['log'] = log_fullpath
    shutil.copy2(args.raw, new_raw_fullpath)
    config[output_hash]['data_raw'] = new_raw_fullpath
    config[output_hash]['figure'] = figure_path
    for binary in binaries:
        sha256 = f"{binary.sha256}"
        config[output_hash][f"{binary.basename}_name"] = binary.basename
        config[output_hash][f"{binary.basename}_fullpath"] = binary.fullpath
        config[output_hash][f"{binary.basename}_sha256"] = sha256

    config[output_hash]['winner'] = binaries[0].basename
    config[output_hash]['winner_mean'] = f"{binaries[0].bayes_mean.statistic:0.5f}"
    config[output_hash]['winner_data'] = log_winner_data
    config[output_hash]['significant'] = str(dispose)
    with open(data_ini_fullpath, 'w') as configfile:
        config.write(configfile)



    #pd.set_option("display.max_rows", None, "display.max_columns", None)
    #pd.set_option('display.max_columns', None)  # or 1000
    #pd.set_option('display.max_rows', None)  # or 1000
    #pd.set_option('display.max_colwidth', None)  # or 199
    #print(pd_all_results)
    #print(f"count: {pd_all_results['milliseconds'].count()}")

    #sns.set_theme()
    #sns.set(rc={"figure.dpi":200, 'savefig.dpi':200})
    sns.set_style("darkgrid", {"axes.facecolor": ".9"})
    sns.set_context("notebook", rc={"grid.linewidth": 0.4, "figure.dpi" : 200, "savefig.dpi" : 200})
    fig = plt.figure(figsize=(3560/200, 1212/200), dpi=200)
    ax1 = fig.add_subplot(111)
    fig.set_dpi(200)

    #ax1 = sns.histplot(ax=ax1, data=pd_all_results, x="milliseconds", hue="binary", element="step", stat="probability", line_kws={'linewidth': 0.5}, kde=True, bins=pd_all_results["milliseconds"].count())
    sns.histplot(ax=ax1, data=pd_all_results, x="milliseconds", hue="binary", stat="count", line_kws={'linewidth': 0.4}, linewidth=0.4, kde=True, element="step", bins="fd")

    ax1.set_ylabel("count", fontsize= 6)
    ax1.set_xlabel("milliseconds", fontsize= 6)
    plt.title("benchsuite", fontsize= 6)
    plt.xticks(fontsize=5)
    plt.yticks(fontsize=5)

    arr = (seg[0][1] for seg in ax1.collections[0].get_paths()[0].iter_segments())
    #y_values = np.array(tuple(arr), dtype = np.float64)
    histplot_y_max = np.amax(np.array(tuple(arr), dtype = np.float64))
    #print(f"histplot_y_max: {histplot_y_max:0.5f}")

    ax1.axvline(x=binaries[0].bayes_mean.statistic, ymin=0, ymax=1, color='red', label=f"{binaries[0].basename} mean", linewidth=0.3, linestyle='--', zorder=5)
    ax1.axvline(x=binaries[1].bayes_mean.statistic, ymin=0, ymax=1, color='blue', label=f"{binaries[1].basename} mean", linewidth=0.3, linestyle='--', zorder=5)

    handles1, labels1 = ax1.get_legend_handles_labels()
    legend = ax1.get_legend()
    handles2 = legend.legendHandles
    #print(handles1)
    #print(labels1)
    #print(handles2)
    handles2 += handles1
    legend.remove()
    ax1.legend(handles2, [f"{binaries[0].basename}", f"{binaries[1].basename}", f"{binaries[0].basename} mean", f"{binaries[1].basename} mean"])

    ax1.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))
    ax1.set_xticks(np.arange(0, pd_all_results["milliseconds"].max()+0.01, 0.01))
    ax1.set_yticks(np.arange(0, histplot_y_max+5, 5))
    ax1.set(xlim=(-0.01, np.around(pd_all_results["milliseconds"].max()+0.01, 2)), ylim=(0,histplot_y_max+5))
    #print(f"ax1.get_xlim(): {ax1.get_xlim()}")
    #print(f"ax1.get_ylim(): {ax1.get_ylim()}")
    #print(f"ax1.get_xticks(): {ax1.get_xticks()}")
    #print(f"ax1.get_yticks(): {ax1.get_yticks()}")


    mean_zoom = np.array(0.0, dtype = np.float64)
    if binaries[0].bayes_mean.statistic > binaries[1].bayes_mean.statistic:
        mean_zoom = np.array(binaries[1].bayes_mean.statistic, dtype = np.float64)
    else:
        mean_zoom = np.array(binaries[0].bayes_mean.statistic, dtype = np.float64)

    ax2 = plt.axes([mean_zoom*5.55, 0.76, .1, .1], facecolor='khaki')
    for spine in ax2.spines.values():
        spine.set_edgecolor('black')
        spine.set_linewidth(0.7)
    ax2.grid(True, color ="black", alpha=0.3)

    ax2.axvline(x=binaries[0].bayes_mean.statistic, ymin=0, ymax=1, color='red', linewidth=0.4, linestyle='--', zorder=5)
    ax2.axvline(x=binaries[1].bayes_mean.statistic, ymin=0, ymax=1, color='blue', linewidth=0.4, linestyle='--', zorder=5)

    ax2.xaxis.set_major_formatter(FormatStrFormatter('%.4f'))
    ax2.set_xticks(np.arange(binaries[0].bayes_mean.statistic-0.001, binaries[0].bayes_mean.statistic+0.001, 0.0005))
    ax2.set(xlim=(binaries[0].bayes_mean.statistic-0.001, binaries[0].bayes_mean.statistic+0.001), ylim=(ax1.get_ylim()[1]-20, ax1.get_ylim()[1]-10))
    ax2.xaxis.set_ticks_position('top')
    ax2.tick_params(pad=0.1, width=0.5, length=2.5, direction='out')
    plt.setp(ax2.get_yticklabels(), visible=False)
    plt.xticks(fontsize=4)
    plt.yticks(fontsize=4)

    #mark_inset(ax1, ax2, loc1=3, loc2=2, fc="none", ec="black", linewidth=0.4, zorder=2)
    mark_inset(ax1, ax2, loc1=4, loc2=2, fc="none", ec="black", linewidth=0.3, zorder=1)

    ax1.autoscale_view()
    plt.tight_layout(pad=0.1)
    fig.subplots_adjust(left=0.020)
    plt.savefig(figure_path, dpi=200, bbox_inches="tight")
    #print(sns.axes_style())
    plt.show()



if __name__ == '__main__':
    main()
